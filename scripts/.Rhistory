formula,
data = training_set
)
#Scaling numeric variables! might not be helpful for tree-based models? is this even remotely justified; should be done probably variable by variable?
this_recipe <-
this_recipe %>%
step_scale(all_numeric_predictors())
this_recipe <- this_recipe %>%
recipes::update_role(ID, new_role = 'ID')
summary(this_recipe)
rf_model2 <-
parsnip::rand_forest() %>%
parsnip::set_mode("classification") %>%
parsnip::set_engine("randomForest")
rf_flow <-
workflows::workflow() %>%
workflows::add_model(rf_model2) %>%
workflows::add_recipe(this_recipe)
rf_flow
rf_fit <-
rf_flow %>%
parsnip::fit(data = training_set)
rf_fit
#Plotting Variable Importance with vip
vip_plot <-
rf_fit %>%
extract_fit_parsnip() %>%
vip::vip()
vip_plot$data
#does it automatically remove low features? does it automatically remove correlating features? since only a few are listed and theyre pretty high up there in terms of value
unimportant_predictors <- vip_plot$data %>%
filter(Importance<= 31)
#attempting to remove the lowest ones for my own curiosity, though 30 is pretty good, removes about 4 variables, depends on run
unimportant_predictors <- unimportant_predictors$Variable
updated_recipe <- this_recipe %>%
step_rm(unimportant_predictors)
#get the formula to update as well
# formula <- update(formula, . ~ . -unimportant_predictors)
#
# formula
updated_recipe <- prep(updated_recipe, training_set)
updated_recipe
#hopefully this updates the goddamn recipe
rf_flow <- rf_flow %>%
workflows::remove_recipe() %>%
workflows::add_recipe(updated_recipe)
rf_fit_updated <- rf_flow %>%
parsnip::fit(data = training_set)
#testing whether anything has changed
predictions2 <- predict(rf_fit_updated, new_data = testing_set)
conf_matrix <- confusionMatrix(predictions2$.pred_class, testing_set$Diagnosis)
conf_matrix
#also horribly inefficiently done
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Precision"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- (2 * precision * recall) / (precision + recall)
specificity <- conf_matrix$byClass["Specificity"]
roc_auc <- roc(testing_set$Diagnosis, as.numeric(predictions$.pred_class))$auc
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
print(paste("Precision:", round(precision, 2)))
print(paste("Recall:", round(recall, 2)))
print(paste("F1 Score:", round(f1_score, 2)))
print(paste("Specificity:", round(specificity, 2)))
print(paste("AUC-ROC:", round(roc_auc, 2)))
# 68.88% accuracy, not much better and undoubtedly bloated?
# likely not really strongly affected by me omitting some values, especially since the vip_plot isnt showing drastically useless variables (i assume 30 is a pretty good estimate)
#checking if the removal actually happened
vip_plot <-
rf_fit_updated %>%
extract_fit_parsnip() %>%
vip::vip()
vip_plot$data
#using my earlier visualizations etc. to hopefully pick the most important features by hand ""domain knowledge""
# hyperparameter grid for tuning
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
#new rf model to be upgraded with feature engineering; to be specific, feature removal
#there is some repetition here since im trying to understand
formula <- Diagnosis ~ speech_rate + average_syllable_duration + n_syllables + p_max + p_iqr + phonation_duration + average_pause_duration + articulation_rate + ID + Study + Trial +duration + total_pause_duration + n_pauses + syllable_duration + syllables_phonation_duration + p_mad + p_coefvar + p_mean + p_sd + p_min + p_median
#i added all predictors, very poorly and stupidly since by hand (to test feature removal and vip)
#also lazy since no random factors right now and no ties to earlier findings
this_recipe <- recipes::recipe(
formula,
data = training_set
)
#Scaling numeric variables! might not be helpful for tree-based models? is this even remotely justified; should be done probably variable by variable?
this_recipe <-
this_recipe %>%
step_scale(all_numeric_predictors())
this_recipe <- this_recipe %>%
recipes::update_role(ID, new_role = 'ID')
summary(this_recipe)
rf_model2 <-
parsnip::rand_forest() %>%
parsnip::set_mode("classification") %>%
parsnip::set_engine("randomForest")
rf_flow <-
workflows::workflow() %>%
workflows::add_model(rf_model2) %>%
workflows::add_recipe(this_recipe)
rf_flow
rf_fit <-
rf_flow %>%
parsnip::fit(data = training_set)
rf_fit
#Plotting Variable Importance with vip
vip_plot <-
rf_fit %>%
extract_fit_parsnip() %>%
vip::vip()
vip_plot$data
#does it automatically remove low features? does it automatically remove correlating features? since only a few are listed and theyre pretty high up there in terms of value
unimportant_predictors <- vip_plot$data %>%
filter(Importance<= 30)
#attempting to remove the lowest ones for my own curiosity, though 30 is pretty good
unimportant_predictors <- unimportant_predictors$Variable
updated_recipe <- this_recipe %>%
step_rm(unimportant_predictors)
#get the formula to update as well
# formula <- update(formula, . ~ . -unimportant_predictors)
#
# formula
updated_recipe <- prep(updated_recipe, training_set)
updated_recipe
#hopefully this updates the goddamn recipe
rf_flow <- rf_flow %>%
workflows::remove_recipe() %>%
workflows::add_recipe(updated_recipe)
rf_fit_updated <- rf_flow %>%
parsnip::fit(data = training_set)
#testing whether anything has changed
predictions2 <- predict(rf_fit_updated, new_data = testing_set)
conf_matrix <- confusionMatrix(predictions2$.pred_class, testing_set$Diagnosis)
conf_matrix
#also horribly inefficiently done
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Precision"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- (2 * precision * recall) / (precision + recall)
specificity <- conf_matrix$byClass["Specificity"]
roc_auc <- roc(testing_set$Diagnosis, as.numeric(predictions$.pred_class))$auc
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
print(paste("Precision:", round(precision, 2)))
print(paste("Recall:", round(recall, 2)))
print(paste("F1 Score:", round(f1_score, 2)))
print(paste("Specificity:", round(specificity, 2)))
print(paste("AUC-ROC:", round(roc_auc, 2)))
# accuracy is roughly the same; not changing
# likely not really strongly affected by me omitting some values, especially since the vip_plot isnt showing drastically useless variables (i assume 30 is a pretty good estimate)
#checking if the removal actually happened
vip_plot <-
rf_fit_updated %>%
extract_fit_parsnip() %>%
vip::vip()
vip_plot$data
rand_seed <- set.seed(123)
knitr::opts_chunk$set(echo = TRUE, cache.extra = rand_seed)
library(pacman)
pacman::p_load(tidyverse, ggplot2, dplyr, caret, parsnip, pROC, ranger, workflows, vip, yardstick, tidymodels, finetune)
#rsample,mlbench, finetune, kernlab, discrim,klaR)
rand_seed <- set.seed(123)
knitr::opts_chunk$set(echo = TRUE, cache.extra = rand_seed) ##how do this??
library(pacman)
pacman::p_load(tidyverse, ggplot2, dplyr, caret, parsnip, pROC, ranger, workflows, vip, yardstick, tidymodels, finetune)
#rsample,mlbench, finetune, kernlab, discrim,klaR)
art_data <- read.table("../data/articulation_data.txt", header = T, fill = T)
p_data <- read.table("../data/pitch_data.txt", header = T, fill = T)
#the 4th version added much later which requires a different clean-up
final_data <- read.table("../data/final_phonation.txt", header = T, fill = T)
#mutating diagnosis for myself
p_data <- p_data %>%
mutate(Diagnosis = if_else(Diagnosis == "control", "CTRL", "SCZ" ))
# #getting rid of useless columns
art_data <- art_data %>%
select(-study, -phonationtime., -X.speakingtime.nsyll., -PauseDuration, -ID, -Study, -Trial, -Diagnosis, -ASD)
#changing the column names to what they ought be for the merge
art_data <- art_data %>%
#these are already present in the data but wrongly named
rename(Trial = X.nsyll)%>%
rename(ID = rate)%>%
rename(Diagnosis = articulation)%>%
rename(Study = X.nsyll.dur.) %>%
#adding the two new values that are missing but can be inferred from data?
mutate(articulationrate = (nsyll/phonationtime))%>%
#is this correct? *ASD (speakingtime/nsyll):* average syllable duration
#but there is no direct speakingtime unless that is speechrate?
mutate(ASD = (speechrate/nsyll)) %>%
#naming to match the naming convention
mutate(Diagnosis = if_else(Diagnosis == "control", "CTRL", "SCZ" ))
#renaming more to make them pitch-distinct and more convenient for me
p_data <- p_data %>%
rename(p_mean = mean) %>%
rename(p_sd = sd) %>%
rename(p_min = min) %>%
rename(p_max = max) %>%
rename(p_median = median) %>%
rename(p_iqr = iqr)%>%
rename(p_mad = mad) %>%
rename(p_coefvar = coefvar)
#renaming more for my own convenience
art_data <- art_data %>%
rename(full_dur = dur)
#should we choose only the relevant columns, are means, mins, maxes, etc. relevant if not listed??
p_data <- p_data %>%
select(ID, Diagnosis, Study, Trial, p_iqr, p_mad, p_coefvar, p_mean, p_sd, p_min, p_max, p_median )
art_data <- art_data %>%
select(ID, Diagnosis, Study, Trial, nsyll, npause, full_dur, phonationtime, speechrate, articulationrate, ASD)
#i assume these to be important as factor; can change it later
p_data$Diagnosis <- as.factor(p_data$Diagnosis)
p_data$Study <- as.factor(p_data$Study)
p_data$Trial <- as.factor(p_data$Trial)
p_data$ID <- as.factor(p_data$ID)
art_data$Diagnosis <- as.factor(art_data$Diagnosis)
art_data$Study <- as.factor(art_data$Study)
art_data$Trial <- as.factor(art_data$Trial)
art_data$ID <- as.factor(art_data$ID)
#original clean up merge, replaced by the maxime update
df <- left_join(p_data, art_data, by = c("ID", "Study", "Trial", "Diagnosis"))
write.csv(df, "../data/merged_data.csv", row.names = FALSE)
final_data <- final_data %>%
mutate(diagnosis = if_else(diagnosis == "control", "CTRL", "SCZ" )) %>%
rename(syllable_duration = X.n_syllables.duration., syllables_phonation_duration = X.n_syllables.phonation_duration.)%>%
rename(Study = study, ID = id, Trial = trial, Diagnosis = diagnosis)%>%
mutate(average_syllable_duration = (syllable_duration/n_syllables))%>%
mutate(average_pause_duration = (pause_duration/n_pauses))%>%
rename(total_pause_duration = pause_duration)
final_data$Diagnosis <- as.factor(final_data$Diagnosis)
final_data$Study <- as.factor(final_data$Study)
final_data$Trial <- as.factor(final_data$Trial)
final_data$ID <- as.factor(final_data$ID)
#as far as I know, we should have the pitch data, even without gender
final_data <- left_join(final_data, p_data, by = c("ID", "Study", "Trial", "Diagnosis"))
write.csv(final_data, "../data/merged_data_from_maxime.csv", row.names = FALSE)
df <- final_data
#there appear to be some extreme values, though I assume they are permitted, hence 3SD as an arbitrary range
df <- df %>%
mutate(across(
where(is.numeric),
~ ifelse(
abs(as.numeric(scale(.x))) > 3,
NA,
.x
)
))
#why are there so many NA values in this dataframe but not the originals? 1900 obs. to 1562 obs. (though some were extreme values omitted)
#could be fixed with ML?
df <- na.omit(df)
#exploring the dataset
df %>%
group_by(Diagnosis) %>%
summarize(Count = n())
#ratio between diagnoses seems balanced enough, though not perfectly 50/50
df %>%
group_by(Diagnosis, Study) %>%
summarize(Count = n())
#fairly balanced, though for some reason big difference between Study 3 for SCZ and CTRL (151 vs 232)
df %>%
group_by(Diagnosis, Trial) %>%
summarize(Count = n())
#appears balanced enough
df %>%
group_by(Diagnosis)%>%
summarize(across(c(speech_rate, phonation_duration, articulation_rate, average_syllable_duration, n_pauses, n_syllables), mean))
#SCZ speak less overall:
#less syllables per second = speechrate
#less speech present in recording = phonationtime (seems significant at a glance!!)
#slightly? less syllabes per second where speech is present = articulationrate
#articulate syllables much slower = ASD (seems significant at a glance!!)
#take slightly? more pauses, though appears fairly similar
#speak less syllables (seems significant at a glance!!)
#doing a quick and dirty t-test to "confirm" the assumptions made at a glance (assuming non-normality so wilcox)
df %>%
summarise(across(where(is.numeric), ~ wilcox.test(.x ~ as.factor(Diagnosis), data = df)$p.value)) %>%
select_if(function(x) any(x < 0.05))
df %>%
summarise(across(where(is.numeric), ~ wilcox.test(.x ~ as.factor(Diagnosis), data = df)$p.value)) %>%
select_if(function(x) any(x > 0.05))
#most columns (outside of pause_duration) are "statistically significant" as in, from different populations (this being a difference between the diagnoses)
#namely what differs statistically is, the duration of the full recording (duration, why different recording times?), the duration of phonation within the recording (phonation_duration), the amount of syllables spoken (n_syllables), amount of pauses taken (n_pauses), average number of syllables per second (speech_rate), the avg duration of syllables spoken, the average number of syllables per second where speech is present (articulation_rate), duration it took to phonate syllables (syllables_phonation_duration) and average_pause_duration (self-explanatory)
#values related to pitch were also significant, such as pitch min and max
#pauses don't differ, making it seem as though this is not a marker for SCZ-style speech. Pitch mean and median are also not statistically significant, meaning that they are roughly the same for both SCZ and CTRL
#this quick t-test would indicate that the speech variables would be able to distinguish one group from the other
#this is very stupidly done here, couldve gotten the mean in the function just fine
means <- df %>%
group_by(Diagnosis) %>%
summarize(mean_ASD = mean(average_syllable_duration), mean_nsyll = mean(n_syllables), mean_npause = mean(n_pauses), mean_pitchmax = mean(p_max), mean_phonationtime = mean(phonation_duration), mean_speechrate = mean(speech_rate), mean_a_pausedur = mean(average_pause_duration), mean_piqr = mean(p_iqr))
plotting_function <- function(df, x, y, fill, means_df, mean_y, altstringx = NULL, altstringy = NULL) {
title_x <- ifelse(!is.null(altstringx), altstringx, gsub("_", " ", x))
title_y <- ifelse(!is.null(altstringy), altstringy, gsub("_", " ", y))
ggplot(df, aes_string(x = x, y = y, fill = fill)) +
geom_violin(trim = TRUE) +
labs(x = title_x, y = title_y) +
ggtitle(paste0(gsub("_", " ", title_x), " by ", gsub("_", " ", title_y))) +
theme_minimal() +
scale_fill_manual(values = c("SCZ" = "gray", "CTRL" = "white")) +
geom_text(data = means_df, aes(label = paste("Mean:", round(.data[[mean_y]], 2))),
x = means_df[[x]], y = means_df[[mean_y]], vjust = -7, hjust = -0.3, size = 4) +
stat_summary(fun = "max", geom = "point", size = 3,
position = position_dodge(width = 0.75), color = "black") +
stat_summary(
fun.data = function(x) {
mean_val <- mean(x)
sd_val <- sd(x)
ymin_val <- max(0, mean_val - sd_val)
ymax_val <- mean_val + sd_val
return(data.frame(y = mean_val, ymin = ymin_val, ymax = ymax_val))
},
geom = "crossbar",
width = 0.02
)
}
plotting_function(df, "Diagnosis", "average_syllable_duration", "Diagnosis", means, "mean_ASD")
#some extreme values in scz, meaning some individual is very slow to speak syllables, overall CTRL appears to have a fat distribution on the bottom of "fast speakers" while this is not the case for SCZ
plotting_function(df, "Diagnosis", "p_max", "Diagnosis", means, "mean_pitchmax", altstringy = "Max pitch")
#pitch appears lower overall on SCZ individuals, with them having a "fatter bottom" i.e. more low pitch speakers. Gender is not taken into account
plotting_function(df, "Diagnosis", "n_syllables", "Diagnosis", means, "mean_nsyll", altstringy = "Number of syllables")
#SCZ has more individuals who spoke less syllables and a lower mean, though there is an extremely high value
plotting_function(df, "Diagnosis", "phonation_duration", "Diagnosis", means, "mean_phonationtime")
#SCZ has a fatter bottom, i.e. more individuals phonating (talking) less throughout the whole recording. There is an extreme value, however
plotting_function(df, "Diagnosis", "speech_rate", "Diagnosis", means, "mean_speechrate")
#SCZ has an overall lower speech rate compared to CTRL
plotting_function(df, "Diagnosis", "average_pause_duration", "Diagnosis", means, "mean_a_pausedur")
#one extreme pause taker in SCZ, higher pause duration overall for SCZ
plotting_function(df, "Diagnosis", "p_iqr", "Diagnosis", means, "mean_piqr", altstringy = "pitch interquartile range")
#much lower pitch for SCZ in terms of Interquartile range
#in other words, the shape of the density plots appears mostly similar between conditions, the SCZ individuals are just differently weighed--such as having a lower max pitch or speech rate
#this is very stupidly done here, couldve gotten the mean in the function just fine
means <- df %>%
group_by(Diagnosis) %>%
summarize(mean_ASD = mean(average_syllable_duration), mean_nsyll = mean(n_syllables), mean_npause = mean(n_pauses), mean_pitchmax = mean(p_max), mean_phonationtime = mean(phonation_duration), mean_speechrate = mean(speech_rate), mean_a_pausedur = mean(average_pause_duration), mean_piqr = mean(p_iqr), mean_pitchmin = mean(p_min))
plotting_function <- function(df, x, y, fill, means_df, mean_y, altstringx = NULL, altstringy = NULL) {
title_x <- ifelse(!is.null(altstringx), altstringx, gsub("_", " ", x))
title_y <- ifelse(!is.null(altstringy), altstringy, gsub("_", " ", y))
ggplot(df, aes_string(x = x, y = y, fill = fill)) +
geom_violin(trim = TRUE) +
labs(x = title_x, y = title_y) +
ggtitle(paste0(gsub("_", " ", title_x), " by ", gsub("_", " ", title_y))) +
theme_minimal() +
scale_fill_manual(values = c("SCZ" = "gray", "CTRL" = "white")) +
geom_text(data = means_df, aes(label = paste("Mean:", round(.data[[mean_y]], 2))),
x = means_df[[x]], y = means_df[[mean_y]], vjust = -7, hjust = -0.3, size = 4) +
stat_summary(fun = "max", geom = "point", size = 3,
position = position_dodge(width = 0.75), color = "black") +
stat_summary(
fun.data = function(x) {
mean_val <- mean(x)
sd_val <- sd(x)
ymin_val <- max(0, mean_val - sd_val)
ymax_val <- mean_val + sd_val
return(data.frame(y = mean_val, ymin = ymin_val, ymax = ymax_val))
},
geom = "crossbar",
width = 0.02
)
}
plotting_function(df, "Diagnosis", "p_min", "Diagnosis", means, "mean_pitchmin", altstringy = "Min pitch")
plotting_function(df, "Diagnosis", "average_syllable_duration", "Diagnosis", means, "mean_ASD")
#some extreme values in scz, meaning some individual is very slow to speak syllables, overall CTRL appears to have a fat distribution on the bottom of "fast speakers" while this is not the case for SCZ
plotting_function(df, "Diagnosis", "p_max", "Diagnosis", means, "mean_pitchmax", altstringy = "Max pitch")
#pitch appears lower overall on SCZ individuals, with them having a "fatter bottom" i.e. more low pitch speakers. Gender is not taken into account
plotting_function(df, "Diagnosis", "p_min", "Diagnosis", means, "mean_pitchmin", altstringy = "Min pitch")
#fatter bottom on SCZ again, lower pitch
plotting_function(df, "Diagnosis", "n_syllables", "Diagnosis", means, "mean_nsyll", altstringy = "Number of syllables")
#SCZ has more individuals who spoke less syllables and a lower mean, though there is an extremely high value
plotting_function(df, "Diagnosis", "phonation_duration", "Diagnosis", means, "mean_phonationtime")
#SCZ has a fatter bottom, i.e. more individuals phonating (talking) less throughout the whole recording. There is an extreme value, however
plotting_function(df, "Diagnosis", "speech_rate", "Diagnosis", means, "mean_speechrate")
#SCZ has an overall lower speech rate compared to CTRL
plotting_function(df, "Diagnosis", "average_pause_duration", "Diagnosis", means, "mean_a_pausedur")
#one extreme pause taker in SCZ, higher pause duration overall for SCZ
plotting_function(df, "Diagnosis", "p_iqr", "Diagnosis", means, "mean_piqr", altstringy = "pitch interquartile range")
#much lower pitch for SCZ in terms of Interquartile range
#in other words, the shape of the density plots appears mostly similar between conditions, the SCZ individuals are just differently weighed--such as having a lower max pitch or speech rate
#omitting Study and Trial since they're not linguistic markers
df <- df %>%
select(-Study, -Trial)
split_df <- initial_split(df, prop = 0.7,strata = Diagnosis)
#this method splits the df into training & testing sets
#just curious
prop_train <- table(training(split_df)$Diagnosis) / length(training(split_df)$Diagnosis)
prop_test <- table(testing(split_df)$Diagnosis) / length(testing(split_df)$Diagnosis)
orig <- table(df$Diagnosis) / nrow(df)
cat("Proportions in Training Set:\n", paste(names(prop_train), prop_train, sep = ": "), "\n\n")
cat("Proportions in Testing Set:\n", paste(names(prop_test), prop_test, sep = ": "), "\n\n")
cat("Proportions in OG set:\n", paste(names(orig), orig, sep = ": "), "\n\n")
#close enough to the split of Diagnosis in the original set, appears validly split into test and training
#still doesnt account for the split of study, training, other variables (could still be a bad training or testing set)
rf_model <- rand_forest(
mode = "classification",
mtry = NULL,
trees = 100,
min_n = 5
) %>%
set_engine("ranger") %>%
fit(Diagnosis ~ . , data = training(split_df))
predictions <- predict(rf_model, new_data = testing(split_df))
conf_matrix <- confusionMatrix(predictions$.pred_class, testing(split_df)$Diagnosis)
conf_matrix
#a bunch of metrics from carot package, probably can be done in a more concise way
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Precision"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- (2 * precision * recall) / (precision + recall)
specificity <- conf_matrix$byClass["Specificity"]
roc_auc <- roc(testing(split_df)$Diagnosis, as.numeric(predictions$.pred_class))$auc
#fix this error
#metrics, dont understand all of them yet, i.e. Precision = True Positives/(true positives + false positives)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
print(paste("Precision:", round(precision, 2)))
print(paste("Recall:", round(recall, 2)))
print(paste("F1 Score:", round(f1_score, 2)))
print(paste("Specificity:", round(specificity, 2)))
print(paste("AUC-ROC:", round(roc_auc, 2)))
#67.74 % accuracy at default? with no feature engineering?
#AUC-ROC 0.67
#doing this like in the lectures
#setting a recipe
this_recipe <-
recipes::recipe(Diagnosis ~ ., data = training(split_df)) %>%
recipes::update_role(ID, new_role = 'ID')
summary(this_recipe)
#recipe set is generic
#Setting Modelling Workflows with parsnip and workflows
rf_model <-
parsnip::rand_forest() %>%
parsnip::set_mode("classification") %>%
parsnip::set_engine("randomForest")
rf_flow1 <-
workflows::workflow() %>%
workflows::add_model(rf_model) %>%
workflows::add_recipe(this_recipe)
rf_flow1
#Fitting the Model with parsnip
rf_fit1 <-
rf_flow1 %>%
parsnip::fit(data = training(split_df))
rf_fit1
#Plotting Variable Importance with vip
vip_plot <-
rf_fit1 %>%
extract_fit_parsnip() %>%
vip::vip()
vip_plot
#Predict Outcome Variable Values
test_results <-
testing(split_df) %>%
as_tibble() %>%
mutate(
predict(rf_fit1, new_data =testing(split_df)),
predict(rf_fit1, new_data = testing(split_df), type = "prob")
)
test_results[1:15, c('Diagnosis', '.pred_class', '.pred_SCZ', '.pred_CTRL')]
#Building the Confusion Matrix with yardstick
conf_mat <-
yardstick::conf_mat(test_results,
truth = Diagnosis,
estimate = .pred_class)
conf_plot <-
autoplot(conf_mat, type = "heatmap") +
scale_fill_gradient(low="#D6EAF8",
high = "#2E86C1") +
theme(
axis.title = element_text(face = 'bold'),
axis.text = element_text(face = 'bold')) +
labs(x = 'TRUTH', y = 'PREDICTION')
conf_plot
#Evaluating Model Predictions with yardstick
eval_metrics <- yardstick::metric_set(yardstick::mcc, yardstick::roc_auc, yardstick::accuracy, yardstick::sens, yardstick::spec, yardstick::f_meas)
#hack-y but works
test_metrics <-
eval_metrics(test_results, Diagnosis,
.pred_SCZ,
estimate = .pred_class)
metrics_plot <-
test_metrics %>%
as_tibble() %>%
ggplot2::ggplot(
aes(y = .metric,
x= .estimate,
label = round(.estimate, digits = 2))) +
geom_segment(aes(x=0, xend=.estimate,
y=.metric, yend=.metric)) +
geom_point(size=5, color="red",
alpha=0.7, shape=21, stroke=2) +
geom_text(aes(y = .metric,
x = .estimate + .05)) +
scale_y_discrete(lim = rev) +
scale_x_continuous(lim = c(0,1),
expand = c(0, 0)) +
labs(x = 'ESTIMATE', y = 'METRIC') +
theme(
axis.title = element_text(face = 'bold'),
axis.text = element_text(face = 'bold'),
)
metrics_plot
#different scores than before even though global set seed?
#new rf model to be upgraded with feature engineering, tripartite best
#also normalization
valid_split <- initial_validation_split(
select(df, !c(Study, Trial)),
prop = c(0.7, 0.15),
strata = Diagnosis
)
rand_seed <- set.seed(123)
knitr::opts_chunk$set(echo = TRUE, cache.extra = rand_seed) ##how do this??
library(pacman)
pacman::p_load(tidyverse, ggplot2, dplyr, caret, parsnip, pROC, ranger, workflows, vip, yardstick, tidymodels, finetune, rsample)
#mlbench, kernlab, discrim,klaR)
