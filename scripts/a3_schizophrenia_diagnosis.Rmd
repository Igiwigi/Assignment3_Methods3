---
title: "Assignment 3 - Diagnosing Schizophrenia from Voice"
subtitle: 'Instructions'
output:
  html_document:
      toc: yes
      number_sections: yes
      toc_float: yes
      theme: united
      highlight: espresso
      css: 'scripts/standard.css'
geometry: margin=1in
knit: (function(inputFile, encoding) {
  browseURL(
    rmarkdown::render(
      inputFile,
      encoding = encoding,
      output_dir = 'documents/',
      output_file = "assignment3_diagnosing_schizophrenia.html"))})
---



```{r setup, include=FALSE}
rand_seed <- set.seed(123)
knitr::opts_chunk$set(echo = TRUE, cache.extra = rand_seed) ##how do this??
library(pacman)
pacman::p_load(plyr, tidyverse, ggplot2, dplyr, caret, parsnip, pROC, ranger, workflows, vip, yardstick, tidymodels, finetune, rsample, discrim, klaR, kernlab)
#mlbench, kernlab, discrim,klaR)
```

This assignment is based on the following paper:

[Parola A et al. 2023. Voice Patterns as Markers of Schizophrenia: Building a Cumulative Generalizable Approach Via a Cross-Linguistic and Meta-analysis Based Investigation. Schizophrenia Bulletin 22(49):S125-S141.](https://doi.org/10.1093/schbul/sbac128)

Individuals with schizophrenia (SCZ) tend to present voice atypicalities. Their tone is described as "inappropriate" voice, sometimes monotone, sometimes croaky. This is important for two reasons. First, voice could constitute a direct window into cognitive, emotional, and social components of the disorder, thus providing a cheap and relatively non-invasive way to support the diagnostic and assessment process (via automated analyses). Secondly, voice atypicalities play an important role in the social impairment experienced by individuals with SCZ, and are thought to generate negative social judgments (of unengaged, slow, unpleasant interlocutors), which can cascade in more negative and less frequent social interactions.

While several studies show significant differences in acoustic features by diagnosis, we want to know whether we can diagnose SCZ in participants only from knowing the features of their voice. 

To that end, the authors collected data from various relevant studies. The latter focused on analyzing voice recordings from people that just got a first diagnosis of schizophrenia, along with a 1:1 case-control sample of participants with matching gender, age, and education. 

Each participant watched several videos (here called trials) of triangles moving across the screen and had to describe them, so you have several recordings per person. 
Along with these files, pitch was recorded once every 10 milliseconds for each participant and various duration-related statistics were also collected (e.g. number of pauses). 

For the purpose of this assignment, studies conducted in languages other than Danish were filtered out.

Your main task for this assignment will be to replicate this research project through the design, fit, and reporting of unsupervised learning methods. More precisely, this assignment will consist in:

  1. Collecting and cleaning the project data
  2. Understanding the data using descriptive statistics
  3. Predicting diagnosis using supervised learning procedures
  4. Discussion on the methods and the results

The following sections will address these objectives in order. You can complete each section in the way that best fits you. However, we remind you that proceeding methodically by segmenting your code in multiple, thematically-organised code chunks will greatly help you mane the whole modeling procedure.

# Collecting and cleaning the project data

There are two different data sets for this assignment:

1. **articulation_data.txt**. This file contains all duration-related data collected from the participants to the different studies included in the project. Here is a short description of its linguistic variables.

  - *nsyll:* number of syllables automatically inferred from the audio
  - *npause:* number of pauses automatically inferred from the audio (absence of human voice longer than 200 milliseconds)
  - *dur (s):* duration (in seconds) of the full recording
  - *phonationtime (s):* duration (in seconds) of the recording where speech is present
  - *speechrate (nsyll/dur):**average number of syllables per second
  - *articulation rate (nsyll/ phonationtime):* average number of syllables per second where speech is present
  - *ASD (speakingtime/nsyll):* average syllable duration

```{r import_articulation_data}
art_data <- read.table("../data/articulation_data.txt", header = T, fill = T)
p_data <- read.table("../data/pitch_data.txt", header = T, fill = T)

#the 4th version added much later which requires a different clean-up
final_data <- read.table("../data/final_phonation.txt", header = T, fill = T)
```


2. **pitch_data.txt**. Aggregated pitch data collected from the participants to the different studies included in the project. Fundamental pitch frequency was recorded for each participant every 10 milliseconds (excluding pauses) and aggregated at the participant trial level with the use of various centrality and dispersion measures. While most column names are self-explanatory, the following might be hard to figure out:

  - *iqr:* Interquartile range
  - *mad:* Mean absolute deviation
  - *coefvar:* Coefficient of variation


```{r cleaning_up_data_for_merge}
#mutating diagnosis for myself
p_data <- p_data %>%
  mutate(Diagnosis = if_else(Diagnosis == "control", "CTRL", "SCZ" ))

# #getting rid of useless columns
art_data <- art_data %>%
  dplyr::select(-study, -phonationtime., -X.speakingtime.nsyll., -PauseDuration, -ID, -Study, -Trial, -Diagnosis, -ASD)

#changing the column names to what they ought be for the merge
art_data <- art_data %>%
  #these are already present in the data but wrongly named
  rename(Trial = X.nsyll)%>%
  rename(ID = rate)%>%
  rename(Diagnosis = articulation)%>%
  rename(Study = X.nsyll.dur.) %>%
  #adding the two new values that are missing but can be inferred from data?
  mutate(articulationrate = (nsyll/phonationtime))%>%
  #is this correct? *ASD (speakingtime/nsyll):* average syllable duration
  #but there is no direct speakingtime unless that is speechrate?
  mutate(ASD = (speechrate/nsyll)) %>%
  #naming to match the naming convention
  mutate(Diagnosis = if_else(Diagnosis == "control", "CTRL", "SCZ" ))

#renaming more to make them pitch-distinct and more convenient for me
p_data <- p_data %>%
rename(p_mean = mean) %>%
rename(p_sd = sd) %>%
rename(p_min = min) %>%
rename(p_max = max) %>%
rename(p_median = median) %>%
rename(p_iqr = iqr)%>%
rename(p_mad = mad) %>%
rename(p_coefvar = coefvar)

#renaming more for my own convenience
art_data <- art_data %>%
rename(full_dur = dur)


#should we choose only the relevant columns, are means, mins, maxes, etc. relevant if not listed??
p_data <- p_data %>%
  dplyr::select(ID, Diagnosis, Study, Trial, p_iqr, p_mad, p_coefvar, p_mean, p_sd, p_min, p_max, p_median )
art_data <- art_data %>%
    dplyr::select(ID, Diagnosis, Study, Trial, nsyll, npause, full_dur, phonationtime, speechrate, articulationrate, ASD)
```

```{r data_refactoring}
#i assume these to be important as factor; can change it later
p_data$Diagnosis <- as.factor(p_data$Diagnosis)
p_data$Study <- as.factor(p_data$Study)
p_data$Trial <- as.factor(p_data$Trial)
p_data$ID <- as.factor(p_data$ID)

art_data$Diagnosis <- as.factor(art_data$Diagnosis)
art_data$Study <- as.factor(art_data$Study)
art_data$Trial <- as.factor(art_data$Trial)
art_data$ID <- as.factor(art_data$ID)
```

After importing the data sets, make sure all common columns and values are named accordingly. Finally, merge the data sets on the appropriate columns, rename columns and values to your liking, and save the resulting data set using a file name and path of your own choosing.

```{r merge_and_save_original}
#original clean up merge, replaced by the maxime update
df <- left_join(p_data, art_data, by = c("ID", "Study", "Trial", "Diagnosis"))
write.csv(df, "../data/merged_data.csv", row.names = FALSE)
```

Fixing the latest dataframe given by Maxime later on:
```{r fix_new_df}
final_data <- final_data %>%
  mutate(diagnosis = if_else(diagnosis == "control", "CTRL", "SCZ" )) %>%
  rename(syllable_duration = X.n_syllables.duration., syllables_phonation_duration = X.n_syllables.phonation_duration.)%>%
  rename(Study = study, ID = id, Trial = trial, Diagnosis = diagnosis)%>%
  mutate(average_syllable_duration = (syllable_duration/n_syllables))%>%
  mutate(average_pause_duration = (pause_duration/n_pauses))%>%
  rename(total_pause_duration = pause_duration)

  
final_data$Diagnosis <- as.factor(final_data$Diagnosis)
final_data$Study <- as.factor(final_data$Study)
final_data$Trial <- as.factor(final_data$Trial)
final_data$ID <- as.factor(final_data$ID)

#as far as I know, we should have the pitch data, even without gender
final_data <- left_join(final_data, p_data, by = c("ID", "Study", "Trial", "Diagnosis"))
write.csv(final_data, "../data/merged_data_from_maxime.csv", row.names = FALSE)


df <- final_data
```

```{r filtering_extreme_values_3SD}
#there appear to be some extreme values, though I assume they are permitted, hence 3SD as an arbitrary range
df <- df %>% 
  mutate(across(
    where(is.numeric),
    ~ ifelse(
      abs(as.numeric(scale(.x))) > 3,
      NA, 
      .x
    )
  ))

#why are there so many NA values in this dataframe but not the originals? 1900 obs. to 1562 obs. (though some were extreme values omitted)
#could be fixed with ML?
df <- na.omit(df)

```

# Understanding the sample using descriptive statistics

In this section, use whatever statistical procedures you think relevant to get a good understanding of the data set, particularly as regards to the differences between linguistic markers of neurotypical and schizophrenic speech. Here as in the following sections, make sure that we understand what you're doing and why you're doing it (you can do this by adding text right before or after the corresponding chunk of code).

Here are some of the things you can do:
  - Describe the data set (number of studies, number of participants, age, gender, clinical and cognitive features of the two groups) and assess whether the groups (schizophrenia and controls) are balanced.
  - Describe the acoustic profile of a schizophrenic voice: which features are different? E.g. People with schizophrenia tend to have high-pitched voice. 

```{r summarize_dataset, warning = F, error= F}
df %>%
  group_by(Diagnosis) %>%
  summarize(Count = n())

df %>%
  group_by(Diagnosis, Study) %>%
  summarize(Count = n())
#fairly balanced, though for some reason big difference between Study 3 for SCZ and CTRL (151 vs 232)

df %>%
  group_by(Diagnosis, Trial) %>%
  summarize(Count = n())
#appears balanced enough

df %>%
  group_by(Diagnosis)%>%
  summarize(across(c(speech_rate, phonation_duration, articulation_rate, average_syllable_duration, n_pauses, n_syllables, p_max), mean))
```
ratio between diagnoses, studies and trials seems balanced enough, though not perfectly 50/50 (for some reason big difference between Study 3 for SCZ and CTRL (151 vs 232)

looking at mean values, the speech markers do seem to differ between SCZ and CTRL
```{r dirty_t-test_dataset, warning = F, error= F}
#doing a quick and dirty t-test to "confirm" the assumptions made at a glance (assuming non-normality so wilcox)
df %>%
  summarise(across(where(is.numeric), ~ wilcox.test(.x ~ as.factor(Diagnosis), data = df)$p.value)) %>%
  select_if(function(x) any(x < 0.05))

df %>%
  summarise(across(where(is.numeric), ~ wilcox.test(.x ~ as.factor(Diagnosis), data = df)$p.value)) %>%
  select_if(function(x) any(x > 0.05))
```
Most columns (outside of pause_duration) are ""statistically significant"" as in, likely to be from 2 different populations (this being a difference between the diagnoses)

Namely what "differs statistically" is, the duration of the full recording (duration, why different recording times?), the duration of phonation within the recording (phonation_duration), the amount of syllables spoken (n_syllables), amount of pauses taken (n_pauses), average number of syllables per second (speech_rate), the avg duration of syllables spoken, the average number of syllables per second where speech is present (articulation_rate), duration it took to phonate syllables (syllables_phonation_duration) and average_pause_duration (self-explanatory)
values related to pitch were also significant, such as pitch min and max

Pauses don't appear to differ so much, making it seem as though this is not a marker for SCZ-style speech. Pitch mean and median are also not statistically significant, meaning that they are roughly the same for both SCZ and CTRL.

This quick t-test would indicate that the speech variables and pitch do differ for the two groups.

That said, I don't know whether I am justified in doing a quick t-test just to see whether the variables are likely to "distinguish" the two groups from each other. Also, not like I checked the assumptions.
```{r summarize_dataset_visually_setup}
#this is very stupidly done here, couldve gotten the mean in the function just fine
means <- df %>%
  group_by(Diagnosis) %>%
  summarize(mean_ASD = mean(average_syllable_duration), mean_nsyll = mean(n_syllables), mean_npause = mean(n_pauses), mean_pitchmax = mean(p_max), mean_phonationtime = mean(phonation_duration), mean_speechrate = mean(speech_rate), mean_a_pausedur = mean(average_pause_duration), mean_piqr = mean(p_iqr), mean_pitchmin = mean(p_min))

plotting_function <- function(df, x, y, fill, means_df, mean_y, altstringx = NULL, altstringy = NULL) {
  title_x <- ifelse(!is.null(altstringx), altstringx, gsub("_", " ", x))
  title_y <- ifelse(!is.null(altstringy), altstringy, gsub("_", " ", y))
  
  ggplot(df, aes_string(x = x, y = y, fill = fill)) +
    geom_violin(trim = TRUE) +
    labs(x = title_x, y = title_y) +
    ggtitle(paste0(gsub("_", " ", title_x), " by ", gsub("_", " ", title_y))) + 
    theme_minimal() +
    scale_fill_manual(values = c("SCZ" = "gray", "CTRL" = "white")) +
    geom_text(data = means_df, aes(label = paste("Mean:", round(.data[[mean_y]], 2))), 
              x = means_df[[x]], y = means_df[[mean_y]], vjust = -7, hjust = -0.3, size = 4) +
    stat_summary(fun = "max", geom = "point", size = 3, 
                 position = position_dodge(width = 0.75), color = "black") +
    stat_summary(
      fun.data = function(x) {
        mean_val <- mean(x)
        sd_val <- sd(x)
        ymin_val <- max(0, mean_val - sd_val)
        ymax_val <- mean_val + sd_val
        return(data.frame(y = mean_val, ymin = ymin_val, ymax = ymax_val))
      },
      geom = "crossbar",
      width = 0.02
    )
}
```


```{r summarize_dataset_visually}
plotting_function(df, "Diagnosis", "average_syllable_duration", "Diagnosis", means, "mean_ASD")

plotting_function(df, "Diagnosis", "p_max", "Diagnosis", means, "mean_pitchmax", altstringy = "Max pitch")

plotting_function(df, "Diagnosis", "p_min", "Diagnosis", means, "mean_pitchmin", altstringy = "Min pitch")

plotting_function(df, "Diagnosis", "n_syllables", "Diagnosis", means, "mean_nsyll", altstringy = "Number of syllables")

plotting_function(df, "Diagnosis", "phonation_duration", "Diagnosis", means, "mean_phonationtime")

plotting_function(df, "Diagnosis", "speech_rate", "Diagnosis", means, "mean_speechrate")

plotting_function(df, "Diagnosis", "average_pause_duration", "Diagnosis", means, "mean_a_pausedur")

plotting_function(df, "Diagnosis", "p_iqr", "Diagnosis", means, "mean_piqr", altstringy = "pitch interquartile range")
```
Visually, it appears that the density plots for both conditions are fairly similar, with the main difference being that SCZ individuals are differently weighed, or are trending towards a direction; for example, they tend to have lower max pitch, higher syllable duration, speak less syllables and phonate (speak) less during the whole recording. 

That said, whether this is distinctive enough to diagnose an individual is hard to say, since the vast majority of SCZ data points seem to overlap with the CTRL individuals in the density plots.

# Predicting diagnosis using supervised learning procedures

We now want to know whether we can automatically diagnose schizophrenia from voice alone. To do this, we will proceed in incremental fashion. We will first start by building a simple random forest model, add an optimized version, and then add a third model based on an algorithm of your choice. Once again, we ask that you connect the different code chunks you create with short descriptive/explanatory text segments that gives us an idea about what you are doing and why you are doing it.

The following packages will be useful to you here:

  - [**tidymodels**](https://tidymodels.tidymodels.org/): “meta-package” for modeling and statistical analysis that shares the underlying design philosophy, grammar, and data structures of the tidyverse.
  - [**rsample**](https://rsample.tidymodels.org/): as infrastructure for resampling data so that models can be assessed and empirically validated.
  -[**groupdata2**][(https://cran.r-project.org/web/packages/groupdata2/vignettes/introduction_to_groupdata2.html)]: an alternative to rsample that allows resampling with deeper grouping
  - [**tune**](https://tune.tidymodels.org/): contains the functions to optimize model hyper-parameters.
  - [**dials**](https://dials.tidymodels.org/): tools to create and manage values of tuning parameters.
  - [**recipes**](https://recipes.tidymodels.org/index.html): a general data preprocessor that can create model matrices incorporating feature engineering, imputation, and other tools.
  - [**workflows**](https://workflows.tidymodels.org/): methods to combine pre-processing steps and models into a single object.
  - [**workflowsets**](https://workflowsets.tidymodels.org/): can create a workflow set that holds multiple workflow objects, allowing users to create and easily fit a large number of models. 
  - [**parsnip**](https://parsnip.tidymodels.org/): a tidy, unified interface to creating models 
  - [**yardstick**](https://yardstick.tidymodels.org/): contains tools for evaluating models

Finally, here are some online resources that can help you with the modeling process:

  - This [**Tidymodels tutorial**](https://www.tidymodels.org/start/) written by the Tidymodels team
  - This [**workshop on Tidymodels**](https://workshops.tidymodels.org/) written by the Tidymodels team
  - This [**workshop on Tidymodels**](https://apreshill.github.io/tidymodels-it/) written by the Posit Team (The company behind RStudio)
  - This [**online course on supervised machine learning**](https://supervised-ml-course.netlify.app/) written by the Tidymodels team

## First phase: Random Forest Model

In this phase, you will build a simple random forest model, by:

  - Splitting the data in training and testing sets
```{r simple_model-training_testing_set}
#omitting Study and Trial since they're not linguistic markers
df <- df %>%
  dplyr::select(-Study, -Trial)

split_df <- initial_split(df, prop = 0.7,strata = Diagnosis)
#this method splits the df into training & testing sets
```


```{r simple_model-checking_train-test_ratio}
#just curious
prop_train <- table(training(split_df)$Diagnosis) / length(training(split_df)$Diagnosis)
prop_test <- table(testing(split_df)$Diagnosis) / length(testing(split_df)$Diagnosis)
orig <- table(df$Diagnosis) / nrow(df)


cat("Proportions in Training Set:\n", paste(names(prop_train), prop_train, sep = ": "), "\n\n")
cat("Proportions in Testing Set:\n", paste(names(prop_test), prop_test, sep = ": "), "\n\n")
cat("Proportions in OG set:\n", paste(names(orig), orig, sep = ": "), "\n\n")
#close enough to the split of Diagnosis in the original set, appears validly split into test and training
#still doesnt account for the split of study, training, other variables (could still be a bad training or testing set)
```

  - Training a random forest model on the training set
```{r training_simple_model}
rf_model <- rand_forest(
  mode = "classification",
  mtry = NULL, 
  trees = 100, 
  min_n = 5
) %>%
  set_engine("ranger") %>% 
  fit(Diagnosis ~ . , data = training(split_df))
```

  - Testing the model's predictions on the testing set
```{r testing_simple_model}
predictions <- predict(rf_model, new_data = testing(split_df))
```

  - Building the confusion matrix
```{r confusion_matrix_simple_model}
conf_matrix <- confusionMatrix(predictions$.pred_class, testing(split_df)$Diagnosis)
conf_matrix
```

  - Compiling performance metrics of your own choosing.
```{r testing_simple_model_metrics}
#a bunch of metrics from carot package, probably can be done in a more concise way
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Precision"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- (2 * precision * recall) / (precision + recall)
specificity <- conf_matrix$byClass["Specificity"]
roc_auc <- roc(testing(split_df)$Diagnosis, as.numeric(predictions$.pred_class))$auc
#fix this error

#metrics, dont understand all of them yet, i.e. Precision = True Positives/(true positives + false positives)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
print(paste("Precision:", round(precision, 2)))
print(paste("Recall:", round(recall, 2)))
print(paste("F1 Score:", round(f1_score, 2)))
print(paste("Specificity:", round(specificity, 2)))
print(paste("AUC-ROC:", round(roc_auc, 2)))
#67.74 % accuracy at default? with no feature engineering?
#AUC-ROC 0.67
```
```{r maxime-example}
#doing this like in the lectures

#setting a recipe
this_recipe <-
recipes::recipe(Diagnosis ~ ., data = training(split_df)) %>%
recipes::update_role(ID, new_role = 'ID')
summary(this_recipe)
#recipe set is generic

#Setting Modelling Workflows with parsnip and workflows
rf_model <-
parsnip::rand_forest() %>%
parsnip::set_mode("classification") %>%
parsnip::set_engine("randomForest")

rf_flow1 <-
workflows::workflow() %>%
workflows::add_model(rf_model) %>%
workflows::add_recipe(this_recipe)
rf_flow1

#Fitting the Model with parsnip
rf_fit1 <-
rf_flow1 %>%
parsnip::fit(data = training(split_df))
rf_fit1

#Plotting Variable Importance with vip
vip_plot <-
rf_fit1 %>%
extract_fit_parsnip() %>%
vip::vip()
vip_plot

#Predict Outcome Variable Values
test_results <-
testing(split_df) %>%
as_tibble() %>%
mutate(
predict(rf_fit1, new_data =testing(split_df)),
predict(rf_fit1, new_data = testing(split_df), type = "prob")
)
test_results[1:15, c('Diagnosis', '.pred_class', '.pred_SCZ', '.pred_CTRL')]

#Building the Confusion Matrix with yardstick
conf_mat <-
yardstick::conf_mat(test_results,
truth = Diagnosis,
estimate = .pred_class)
conf_plot <-
autoplot(conf_mat, type = "heatmap") +
scale_fill_gradient(low="#D6EAF8",
high = "#2E86C1") +
theme(
axis.title = element_text(face = 'bold'),
axis.text = element_text(face = 'bold')) +
labs(x = 'TRUTH', y = 'PREDICTION')
conf_plot

#Evaluating Model Predictions with yardstick

eval_metrics <- yardstick::metric_set(yardstick::mcc, yardstick::roc_auc, yardstick::accuracy, yardstick::sens, yardstick::spec, yardstick::f_meas)
#hack-y but works

test_metrics <-
eval_metrics(test_results, Diagnosis,
.pred_SCZ,
estimate = .pred_class)

metrics_plot <-
test_metrics %>%
as_tibble() %>%
ggplot2::ggplot(
aes(y = .metric,
x= .estimate,
label = round(.estimate, digits = 2))) +
geom_segment(aes(x=0, xend=.estimate,
y=.metric, yend=.metric)) +
geom_point(size=5, color="red",
alpha=0.7, shape=21, stroke=2) +
geom_text(aes(y = .metric,
x = .estimate + .05)) +
scale_y_discrete(lim = rev) +
scale_x_continuous(lim = c(0,1),
expand = c(0, 0)) +
labs(x = 'ESTIMATE', y = 'METRIC') +
theme(
axis.title = element_text(face = 'bold'),
axis.text = element_text(face = 'bold'),
)
metrics_plot
#different scores than before even though global set seed?
```

## Second phase: Forest Engineering

In this section, you will try to optimize the performance of the model developed in the previous phase by adding a new random forest model, upgraded with feature engineering and parameter tuning procedures of your own choosing.
```{r curious_about_correlations}
#curious about the correlation of the numeric variables; many are derived from the same thing, so might be bad to have? though could also help the algorithm; helpful redundancy?
numeric_df <- df[sapply(df, is.numeric) & !sapply(df, is.factor)]
correlation_matrix <- cor(numeric_df)

hc <- findCorrelation(correlation_matrix, cutoff=0.5) #setting the cut off fairly high
not_cor_variables <- df[c(hc)]
not_cor_variables <-  not_cor_variables[, c("ID", "Diagnosis", setdiff(names(not_cor_variables), c("ID", "Diagnosis")))] #reordering
not_cor_variables
#these could be good ones to keep? though i dont know fully if having correlated variables is bad? also doesnt the ML automatically figure this out, or?

print(paste0(paste(names(df)[-hc], collapse = ", "), " - These should be omitted due to high correlation? Is this even remotely correct?"))

#formula <- Diagnosis ~ ID + phonation_duration + p_sd + p_min + p_mad + articulation_rate + p_coefvar + n_syllables + duration + average_pause_duration + p_iqr + speech_rate + syllables_phonation_duration?
#would it even make a difference? have I done this correctly?

to_remove <- names(df[-hc])
```

```{r removing_corr_from_recipe}
#trying to omit the correlated predictors from earlier, wound up creating a new recipe since step_rm was finnicky
predictors <- this_recipe$var_info$variable
new_predictors <- setdiff(predictors, c(to_remove, "Diagnosis"))


cor_removed_recipe <-
  recipe(
    Diagnosis ~ ID + duration + phonation_duration + n_syllables + speech_rate + articulation_rate + syllables_phonation_duration + average_pause_duration + p_iqr + p_mad + p_coefvar + p_sd + p_min,
    data = training(split_df)
  ) %>% #had to do by hand unfortunately, couldnt figure it out otherwise
  step_normalize(all_numeric_predictors(),-all_outcomes()) %>%
  step_YeoJohnson(all_numeric_predictors(),-all_outcomes()) %>%
  update_role(ID, new_role = "ID")

summary(cor_removed_recipe)
#this seems to make no difference, really
```

```{r feature_engineering-1}
#new rf model to be upgraded with feature engineering, tripartite best
#also normalization

valid_split <- initial_validation_split(
   df, 
  prop = c(0.7, 0.15),
  strata = Diagnosis
)
#splits into test, training AND validation

cor_removed_recipe <-
  recipe(
    Diagnosis ~ ID + duration + phonation_duration + n_syllables + speech_rate + articulation_rate + syllables_phonation_duration + average_pause_duration + p_iqr + p_mad + p_coefvar + p_sd + p_min,
    data = training(valid_split) #using validation here
  ) %>% 
  step_normalize(all_numeric_predictors(),-all_outcomes()) %>%
  update_role(ID, new_role = "ID")
#normalizing the variables, center and scale them, since good for unsupervised learning? but might need to be better justified
#In general you should be careful about using -all_outcomes() if a ⁠*_predictors()⁠ selector would do what you want.


rf_flow <- workflow() %>% 
  add_model(rand_forest(
    mode = "classification", 
    engine = "randomForest",
    trees = tune(), #tune these
    min_n = tune()
  )) %>% 
  add_recipe(cor_removed_recipe)

tripartite <- tune_grid(
  rf_flow, 
  vfold_cv(validation(valid_split)), 
  grid = grid_max_entropy(extract_parameter_set_dials(rf_flow), size = 10)
)
tripartite %>% collect_metrics()
#doing tripartite now
```


```{r feature_engineering-2}
final_fit <- last_fit(
  finalize_workflow(rf_flow, select_best(tripartite, "roc_auc")), 
  valid_split, 
  add_validation_set = TRUE
)
#using the best tripartite and finalizing workplace, using validation set

#Plotting Variable Importance with vip
vip_plot <-
final_fit %>%
extract_fit_parsnip() %>%
vip::vip()
vip_plot

predicted <- final_fit %>%
  pull(.predictions) %>%
  bind_cols() %>%
  mutate(
    Predicted = .pred_class,
    Prob_SCZ = .pred_SCZ,
    type = "prob"
  )

conf_matrix <- confusionMatrix(predicted$Predicted, testing(valid_split)$Diagnosis)
conf_matrix

predicted
#accuracy not much better?
```


## Third phase: Another Algorithm

For this final part, add a supervised algorithm to the workflow set and compare its performance to the previous ones. Here again, you are free to choose any algorithm, but it its important that we know what you're doing and why you are doing it. In other words, tell us a bit about the algorithm you're using and why you chose it.

For a detailed list of the model types, engines, and arguments that can be used with the tidymodels framework, have a look here https://www.tidymodels.org/find/parsnip/#models


```{r anova_racing_kernlab}
#racing method with anova, not tuning the recipe though?

training_folds <- vfold_cv(df, v = 10) #splitting into folds for training

m1 <-
  svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
  set_engine("kernlab") %>%
  set_mode("classification")

m1_grid <-
  m1 %>%
  hardhat::extract_parameter_set_dials() %>% 
  grid_latin_hypercube(size = 25)

wflow <-
  workflow() %>%
  add_model(m1) %>%
  add_recipe(cor_removed_recipe)

anova_race <- wflow %>% tune_race_anova(resamples = training_folds, grid = m1_grid)
```


```{r anova_racing_kernlab_2}
show_best(anova_race, metric = "roc_auc", n = 2)
plot_race(anova_race)

best_model <- select_best(anova_race, metric = "roc_auc")

final_rf <- finalize_model(
  m1,
  best_model
) #am i using this bit correctly?

final_res <- cor_removed_recipe %>%
  workflow() %>%
  add_model(final_rf)

final_fit  <- final_res %>%
last_fit(split_df)

final_metrics <- final_fit %>%
  collect_metrics()

final_metrics
#better performance than the later model? or more or less the same?

predicted <- data.frame(final_fit$.predictions)

conf_matrix <-
  confusionMatrix(
    data = predicted$.pred_class,  # Accessing the predictions column
    reference = predicted$Diagnosis,
    positive = "SCZ"
  )
print(conf_matrix)
```

```{r anova_racing_klaR_preprocessing}
rda_spec <- discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %>%
    set_engine("klaR")%>%
  set_mode("classification")

ctrl <- control_race(verbose_elim = TRUE)

grid_anova <- rda_spec %>%
    tune_race_anova(cor_removed_recipe, resamples = training_folds, grid = 10, control = ctrl)

show_best(grid_anova, metric = "roc_auc", n = 2)
plot_race(grid_anova)

best_model <- select_best(grid_anova, metric = "roc_auc")

final_model <- finalize_model(
  rda_spec,
  best_model
) #is this a correct way to finalize?

final_res <- cor_removed_recipe %>%
  workflow() %>%
  add_model(final_model)

final_fit  <- final_res %>%
last_fit(split_df)

predicted <- data.frame(final_fit$.predictions)

conf_matrix <-
  confusionMatrix(
    data = predicted$.pred_class,  # Accessing the predictions column
    reference = predicted$Diagnosis,
    positive = "SCZ"
  )
print(conf_matrix)
```

```{r optimize_model_gbm_failure?}
pacman::p_load("gbm") #trying out gbm

ctrl <- trainControl(method = "cv", 
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)


model <- train(
  cor_removed_recipe, 
  data = training(split_df), 
  method = "gbm",
  trControl = ctrl,
  tuneLength = 5,
  metric = "ROC",
  verbose = FALSE
)

predictions <- predict(model, newdata = testing(split_df))

predicted_labels <- factor(predictions, levels = c("SCZ", "CTRL"))
actual_labels <- factor(testing(split_df)$Diagnosis, levels = c("SCZ", "CTRL"))

conf_matrix <- confusionMatrix(data = predicted_labels, reference = actual_labels)
conf_matrix
```


```{r optimize_model_svmPoly}
svmPoly_model <- train(
  cor_removed_recipe, #normalization; is that a smart choice? was removing correlates ok? did it make a difference
  data = training(split_df),
  method = "svmPoly",
  trControl = trainControl(method = "cv"), #since cv, couldnt i use the whole data?
  tuneLength = 3, #lowered it
  verbose = F
)
plot(svmPoly_model)
max(svmPoly_model$results$Accuracy)

svmPoly_predict <- predict(svmPoly_model, newdata = testing(split_df))
conf_matrix <- confusionMatrix(data = svmPoly_predict, reference = testing(split_df)$Diagnosis, positive = "SCZ")
print(conf_matrix)
```



```{r svmPoly_as_a_workflow}
final_model_svm <- 
  svm_rbf() %>% #using svm
  set_engine("kernlab") %>%
parsnip::set_mode("classification")

final_wf <- workflow() %>%
  add_model(final_model_svm)%>%
  add_recipe(cor_removed_recipe)

rf_fit <- final_wf %>% fit(training(split_df))

max(model$results$ROC) 
#names(getModelInfo())
model$results

predict <- predict(model, newdata = testing(split_df))
conf_matrix <- confusionMatrix(data = predict, reference = testing(split_df)$Diagnosis, positive = "SCZ")
print(conf_matrix)

predict <- predict(rf_fit, new_data = testing(split_df))
conf_matrix <- confusionMatrix(data = predict$.pred_class, reference = testing(split_df)$Diagnosis, positive = "SCZ")
print(conf_matrix)
```


# Discussion: Methodology and Results

Finally, briefly summarize and discuss the methodological choices you've made throughout as well as the results obtained at the different modeling stages. In particular, I would like to get your input as regards to the following questions:

I messed around a lot with my models and as a result I have a lot of filler and some of it may be messy and even incorrectly done. 

Anyway, I attempted the following:

I figured the parameters available to us likely have high correlation and that this might be a problem, as the model would simply overfit on the training data parameters in such a way where it does not have a clear understanding of how the variables appear in nature; i.e. it cannot distinguish the real influence/importance of each parameter. As such, I tried to cull them by calculating the correlations and omitting any over 0.5, and making these the predictors. I did this in a way that might not be entirely optimal; I assume a ML solution with tuning the recipe would've sufficed better.

I normalized the recipe predictors and used yeojohnson as I figured that would help with the lack of normality the data had, squishing it into a narrower range. Perhaps I should've done more pre-processing.

Afterwards, I messed about with various ML methods, such as tuning a tripartite and picking the best model, anova racing using kernlab and klaR engines, gbm (gradient boosting machine), svmPoly (support vector machine model). 
They gave mostly the same results of 60-65% accuracy and similar ROC.
It's possible that I was doing them incorrectly, or that one of my pre-processing steps, such as the recipe I chose, may have lessened the data/training quality.

  - Based on the performance evaluation of your models, do you think the second and third phase of the third section were worth the extra effort? Was any model successful in diagnosing schizophrenia from voice?

It appears that I am not yet capable of actually creating or tuning a model to be better than the default; i.e. I was stuck at around 64% throughout my attempts, which was not better than what I had in the 2nd part. I assume there's something I'm still missing or that the act of tuning and training a model takes extensive trial and error alongside statistical knowledge--that is, choices that are justified with more than idle curiosity and have some sort of knowledge of the data distribution or statistical methods behind them (the latter two I am still lacking in). That said, it could be not worth the effort for this task--I'm not sure.

  - How do the predictive models you built relate to the descriptive analysis conducted in the second section?

I struggled with tying the predictive models to the descriptive analysis, but I did attempt to remove the features that I imagined to be less useful for prediction; that is, the features that correlated heavily with each other. Many of these were "not statistically significant" when I did a quick wilcox on them earlier on. I'm not sure whether a quick statistical significance test is warranted, I just figured the "statistically likely" values were the ones likely to differentiate the groups from each other, and therefore be useful for a classification task.

It does appear that I may have been correct about the SCZ and CTRL speech and pitch parameters overlapping too much on the density plot for it to be a slam dunk classification task.

  - What is the explanatory scope of the analyses and procedures you conducted here, if any?
From the visual analyses alone, it is rather apparent that schizophrenic and non-schizophrenic individuals have more or less the same "distribution" of vocal characteristics outside of schizophrenics "trending towards the bottom" on variables such as pitch and number of syllables spoken. It is very likely that, as a result, it is rather difficult to distinguish one from the other unless you happen to have a schizophrenic with a very "stereotypically extreme" distribution of pitch and speech characteristics. As such, I'm not sure whether speech characteristics are enough alone for the sake of distinguishing the two from each other (reliably anyway).

While it's possible my models were all just poorly optimized and handled, it seems to me that a percentage of much over 70% would be overfitting on the training data/data available to the study. I suppose it could still be used as one of many markers of schizophrenia though, just not as a standalone one.