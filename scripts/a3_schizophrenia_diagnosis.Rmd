---
title: "Assignment 3 - Diagnosing Schizophrenia from Voice"
subtitle: 'Instructions'
output:
  html_document:
      toc: yes
      number_sections: yes
      toc_float: yes
      theme: united
      highlight: espresso
      css: 'scripts/standard.css'
geometry: margin=1in
knit: (function(inputFile, encoding) {
  browseURL(
    rmarkdown::render(
      inputFile,
      encoding = encoding,
      output_dir = 'documents/',
      output_file = "assignment3_diagnosing_schizophrenia.html"))})
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
#install.packages(c("tidyverse", "ggplot2", "dplyr", "caret", "parsnip", "pROC", "ranger", "pacman"))
pacman::p_load(tidyverse, ggplot2, dplyr, caret, parsnip, pROC, ranger)
```

This assignment is based on the following paper:

[Parola A et al. 2023. Voice Patterns as Markers of Schizophrenia: Building a Cumulative Generalizable Approach Via a Cross-Linguistic and Meta-analysis Based Investigation. Schizophrenia Bulletin 22(49):S125-S141.](https://doi.org/10.1093/schbul/sbac128)

Individuals with schizophrenia (SCZ) tend to present voice atypicalities. Their tone is described as "inappropriate" voice, sometimes monotone, sometimes croaky. This is important for two reasons. First, voice could constitute a direct window into cognitive, emotional, and social components of the disorder, thus providing a cheap and relatively non-invasive way to support the diagnostic and assessment process (via automated analyses). Secondly, voice atypicalities play an important role in the social impairment experienced by individuals with SCZ, and are thought to generate negative social judgments (of unengaged, slow, unpleasant interlocutors), which can cascade in more negative and less frequent social interactions.

While several studies show significant differences in acoustic features by diagnosis, we want to know whether we can diagnose SCZ in participants only from knowing the features of their voice. 

To that end, the authors collected data from various relevant studies. The latter focused on analyzing voice recordings from people that just got a first diagnosis of schizophrenia, along with a 1:1 case-control sample of participants with matching gender, age, and education. 

Each participant watched several videos (here called trials) of triangles moving across the screen and had to describe them, so you have several recordings per person. 
Along with these files, pitch was recorded once every 10 milliseconds for each participant and various duration-related statistics were also collected (e.g. number of pauses). 

For the purpose of this assignment, studies conducted in languages other than Danish were filtered out.

Your main task for this assignment will be to replicate this research project through the design, fit, and reporting of unsupervised learning methods. More precisely, this assignment will consist in:

  1. Collecting and cleaning the project data
  2. Understanding the data using descriptive statistics
  3. Predicting diagnosis using supervised learning procedures
  4. Discussion on the methods and the results

The following sections will address these objectives in order. You can complete each section in the way that best fits you. However, we remind you that proceeding methodically by segmenting your code in multiple, thematically-organised code chunks will greatly help you mane the whole modeling procedure.

# Collecting and cleaning the project data

There are two different data sets for this assignment:

1. **articulation_data.txt**. This file contains all duration-related data collected from the participants to the different studies included in the project. Here is a short description of its linguistic variables.

  - *nsyll:* number of syllables automatically inferred from the audio
  - *npause:* number of pauses automatically inferred from the audio (absence of human voice longer than 200 milliseconds)
  - *dur (s):* duration (in seconds) of the full recording
  - *phonationtime (s):* duration (in seconds) of the recording where speech is present
  - *speechrate (nsyll/dur):**average number of syllables per second
  - *articulation rate (nsyll/ phonationtime):* average number of syllables per second where speech is present
  - *ASD (speakingtime/nsyll):* average syllable duration

```{r import_articulation_data}
art_data <- read.table("../data/articulation_data.txt", header = T, fill = T)
p_data <- read.table("../data/pitch_data.txt", header = T, fill = T)

#the 4th version added much later which requires a different clean-up
final_data <- read.table("../data/final_phonation.txt", header = T, fill = T)
```


2. **pitch_data.txt**. Aggregated pitch data collected from the participants to the different studies included in the project. Fundamental pitch frequency was recorded for each participant every 10 milliseconds (excluding pauses) and aggregated at the participant trial level with the use of various centrality and dispersion measures. While most column names are self-explanatory, the following might be hard to figure out:

  - *iqr:* Interquartile range
  - *mad:* Mean absolute deviation
  - *coefvar:* Coefficient of variation


```{r cleaning_up_data_for_merge}
#mutating diagnosis for myself
p_data <- p_data %>%
  mutate(Diagnosis = if_else(Diagnosis == "control", "CTRL", "SCZ" ))

# #getting rid of useless columns
art_data <- art_data %>%
  select(-study, -phonationtime., -X.speakingtime.nsyll., -PauseDuration, -ID, -Study, -Trial, -Diagnosis, -ASD)

#changing the column names to what they ought be for the merge
art_data <- art_data %>%
  #these are already present in the data but wrongly named
  rename(Trial = X.nsyll)%>%
  rename(ID = rate)%>%
  rename(Diagnosis = articulation)%>%
  rename(Study = X.nsyll.dur.) %>%
  #adding the two new values that are missing but can be inferred from data?
  mutate(articulationrate = (nsyll/phonationtime))%>%
  #is this correct? *ASD (speakingtime/nsyll):* average syllable duration
  #but there is no direct speakingtime unless that is speechrate?
  mutate(ASD = (speechrate/nsyll)) %>%
  #naming to match the naming convention
  mutate(Diagnosis = if_else(Diagnosis == "control", "CTRL", "SCZ" ))

#renaming more to make them pitch-distinct and more convenient for me
p_data <- p_data %>%
rename(p_mean = mean) %>%
rename(p_sd = sd) %>%
rename(p_min = min) %>%
rename(p_max = max) %>%
rename(p_median = median) %>%
rename(p_iqr = iqr)%>%
rename(p_mad = mad) %>%
rename(p_coefvar = coefvar)

#renaming more for my own convenience
art_data <- art_data %>%
rename(full_dur = dur)


#should we choose only the relevant columns, are means, mins, maxes, etc. relevant if not listed??
p_data <- p_data %>%
  select(ID, Diagnosis, Study, Trial, p_iqr, p_mad, p_coefvar, p_mean, p_sd, p_min, p_max, p_median )
art_data <- art_data %>%
    select(ID, Diagnosis, Study, Trial, nsyll, npause, full_dur, phonationtime, speechrate, articulationrate, ASD)
```

```{r data_refactoring}
#i assume these to be important as factor; can change it later
p_data$Diagnosis <- as.factor(p_data$Diagnosis)
p_data$Study <- as.factor(p_data$Study)
p_data$Trial <- as.factor(p_data$Trial)
p_data$ID <- as.factor(p_data$ID)

art_data$Diagnosis <- as.factor(art_data$Diagnosis)
art_data$Study <- as.factor(art_data$Study)
art_data$Trial <- as.factor(art_data$Trial)
art_data$ID <- as.factor(art_data$ID)
```

After importing the data sets, make sure all common columns and values are named accordingly. Finally, merge the data sets on the appropriate columns, rename columns and values to your liking, and save the resulting data set using a file name and path of your own choosing.

```{r merge_and_save_original}
#original clean up merge, replaced by the maxime update
df <- left_join(p_data, art_data, by = c("ID", "Study", "Trial", "Diagnosis"))
write.csv(df, "../data/merged_data.csv", row.names = FALSE)
```

Fixing the dataframe given by Maxime later on (I hope I'm doing it correctly; I am very confused)
```{r fix_new_df}
final_data <- final_data %>%
  mutate(diagnosis = if_else(diagnosis == "control", "CTRL", "SCZ" )) %>%
  rename(syllable_duration = X.n_syllables.duration., syllables_phonation_duration = X.n_syllables.phonation_duration.)%>%
  rename(Study = study, ID = id, Trial = trial, Diagnosis = diagnosis)%>%
  mutate(average_syllable_duration = (syllable_duration/n_syllables))%>%
  mutate(average_pause_duration = (pause_duration/n_pauses))%>%
  rename(total_pause_duration = pause_duration)

  
final_data$Diagnosis <- as.factor(final_data$Diagnosis)
final_data$Study <- as.factor(final_data$Study)
final_data$Trial <- as.factor(final_data$Trial)
final_data$ID <- as.factor(final_data$ID)

#why are there so many NA values in this dataframe but not the originals? 1900 obs. to 1761 obs.
final_data <- na.omit(final_data)

#as far as I know, we should have the pitch data, even without gender
final_data <- left_join(final_data, p_data, by = c("ID", "Study", "Trial", "Diagnosis"))
write.csv(final_data, "../data/merged_data_from_maxime.csv", row.names = FALSE)


df <- final_data
```


# Understanding the sample using descriptive statistics

In this section, use whatever statistical procedures you think relevant to get a good understanding of the data set, particularly as regards to the differences between linguistic markers of neurotypical and schizophrenic speech. Here as in the following sections, make sure that we understand what you're doing and why you're doing it (you can do this by adding text right before or after the corresponding chunk of code).

Here are some of the things you can do:
  - Describe the data set (number of studies, number of participants, age, gender, clinical and cognitive features of the two groups) and assess whether the groups (schizophrenia and controls) are balanced.
  - Describe the acoustic profile of a schizophrenic voice: which features are different? E.g. People with schizophrenia tend to have high-pitched voice. 

```{r summarize_dataset, warning = F, error= F}
#exploring the dataset 

df %>%
  group_by(Diagnosis) %>%
  summarize(Count = n())
#ratio between diagnoses seems balanced enough, though not perfectly 50/50

df %>%
  group_by(Diagnosis, Study) %>%
  summarize(Count = n())
#fairly balanced, though for some reason big difference between Study 3 for SCZ and CTRL (151 vs 232)

df %>%
  group_by(Diagnosis, Trial) %>%
  summarize(Count = n())
#appears balanced enough

df %>%
  group_by(Diagnosis)%>%
  summarize(across(c(speech_rate, phonation_duration, articulation_rate, average_syllable_duration, n_pauses, n_syllables), mean))
#SCZ speak less overall: 
#less syllables per second = speechrate
#less speech present in recording = phonationtime (seems significant at a glance!!)
#slightly? less syllabes per second where speech is present = articulationrate
#articulate syllables much slower = ASD (seems significant at a glance!!)
#take slightly? more pauses, though appears fairly similar
#speak less syllables (seems significant at a glance!!)

#doing a quick and dirty t-test to "confirm" the assumptions made at a glance (assuming non-normality so wilcox)
df %>%
  summarise(across(where(is.numeric), ~ wilcox.test(.x ~ as.factor(Diagnosis), data = df)$p.value)) %>%
  select_if(function(x) any(x < 0.05))

df %>%
  summarise(across(where(is.numeric), ~ wilcox.test(.x ~ as.factor(Diagnosis), data = df)$p.value)) %>%
  select_if(function(x) any(x > 0.05))

#most columns (outside of pause_duration) are "statistically significant" as in, from different populations (this being a difference between the diagnoses)

#namely what differs statistically is, the duration of the full recording (duration, why different recording times?), the duration of phonation within the recording (phonation_duration), the amount of syllables spoken (n_syllables), amount of pauses taken (n_pauses), average number of syllables per second (speech_rate), the avg duration of syllables spoken, the average number of syllables per second where speech is present (articulation_rate), duration it took to phonate syllables (syllables_phonation_duration) and average_pause_duration (self-explanatory)
#values related to pitch were also significant, such as pitch min and max

#total pause duration did not differ, neither did pitch mean nor median, signifying that, on average, both groups have the same median and mean values for pitch? and tend to take the same amount of pauses overall

#this quick t-test would indicate that the speech variables would be able to distinguish one group from the other
```
```{r summarize_dataset_visually_setup}
#this is very stupidly done here, couldve gotten the mean in the function just fine
means <- df %>%
  group_by(Diagnosis) %>%
  summarize(mean_ASD = mean(average_syllable_duration), mean_nsyll = mean(n_syllables), mean_npause = mean(n_pauses), mean_pitchmax = mean(p_max), mean_phonationtime = mean(phonation_duration), mean_speechrate = mean(speech_rate), mean_a_pausedur = mean(average_pause_duration), mean_piqr = mean(p_iqr))

plotting_function <- function(df, x, y, fill, means_df, mean_y, altstringx = NULL, altstringy = NULL) {
  title_x <- ifelse(!is.null(altstringx), altstringx, gsub("_", " ", x))
  title_y <- ifelse(!is.null(altstringy), altstringy, gsub("_", " ", y))
  
  ggplot(df, aes_string(x = x, y = y, fill = fill)) +
    geom_violin(trim = TRUE) +
    labs(x = title_x, y = title_y) +
    ggtitle(paste0(gsub("_", " ", title_x), " by ", gsub("_", " ", title_y))) + 
    theme_minimal() +
    scale_fill_manual(values = c("SCZ" = "gray", "CTRL" = "white")) +
    geom_text(data = means_df, aes(label = paste("Mean:", round(.data[[mean_y]], 2))), 
              x = means_df[[x]], y = means_df[[mean_y]], vjust = -7, hjust = -0.3, size = 4) +
    stat_summary(fun = "max", geom = "point", size = 3, 
                 position = position_dodge(width = 0.75), color = "black") +
    stat_summary(
      fun.data = function(x) {
        mean_val <- mean(x)
        sd_val <- sd(x)
        ymin_val <- max(0, mean_val - sd_val)
        ymax_val <- mean_val + sd_val
        return(data.frame(y = mean_val, ymin = ymin_val, ymax = ymax_val))
      },
      geom = "crossbar",
      width = 0.02
    )
}
```


```{r summarize_dataset_visually}
plotting_function(df, "Diagnosis", "average_syllable_duration", "Diagnosis", means, "mean_ASD")
#some extreme values in scz, meaning some individual is very slow to speak syllables, overall CTRL appears to have a fat distribution on the bottom of "fast speakers" while this is not the case for SCZ

plotting_function(df, "Diagnosis", "p_max", "Diagnosis", means, "mean_pitchmax", altstringy = "Max pitch")
#pitch appears lower overall on SCZ individuals, with them having a "fatter bottom" i.e. more low pitch speakers. Gender is not taken into account

plotting_function(df, "Diagnosis", "n_syllables", "Diagnosis", means, "mean_nsyll", altstringy = "Number of syllables")
#SCZ has more individuals who spoke less syllables and a lower mean, though there is an extremely high value

plotting_function(df, "Diagnosis", "phonation_duration", "Diagnosis", means, "mean_phonationtime")
#SCZ has a fatter bottom, i.e. more individuals phonating (talking) less throughout the whole recording. There is an extreme value, however

plotting_function(df, "Diagnosis", "speech_rate", "Diagnosis", means, "mean_speechrate")
#SCZ has an overall lower speech rate compared to CTRL

plotting_function(df, "Diagnosis", "average_pause_duration", "Diagnosis", means, "mean_a_pausedur")
#one extreme pause taker in SCZ, higher pause duration overall for SCZ

plotting_function(df, "Diagnosis", "p_iqr", "Diagnosis", means, "mean_piqr", altstringy = "pitch interquartile range")
#much lower pitch for SCZ in terms of Interquartile range

#in other words, the shape of the density plots appears mostly similar between conditions, the SCZ individuals are just differently weighed--such as having a lower max pitch or speech rate
```


# Predicting diagnosis using supervised learning procedures

We now want to know whether we can automatically diagnose schizophrenia from voice alone. To do this, we will proceed in incremental fashion. We will first start by building a simple random forest model, add an optimized version, and then add a third model based on an algorithm of your choice. Once again, we ask that you connect the different code chunks you create with short descriptive/explanatory text segments that gives us an idea about what you are doing and why you are doing it.

The following packages will be useful to you here:

  - [**tidymodels**](https://tidymodels.tidymodels.org/): “meta-package” for modeling and statistical analysis that shares the underlying design philosophy, grammar, and data structures of the tidyverse.
  - [**rsample**](https://rsample.tidymodels.org/): as infrastructure for resampling data so that models can be assessed and empirically validated.
  -[**groupdata2**][(https://cran.r-project.org/web/packages/groupdata2/vignettes/introduction_to_groupdata2.html)]: an alternative to rsample that allows resampling with deeper grouping
  - [**tune**](https://tune.tidymodels.org/): contains the functions to optimize model hyper-parameters.
  - [**dials**](https://dials.tidymodels.org/): tools to create and manage values of tuning parameters.
  - [**recipes**](https://recipes.tidymodels.org/index.html): a general data preprocessor that can create model matrices incorporating feature engineering, imputation, and other tools.
  - [**workflows**](https://workflows.tidymodels.org/): methods to combine pre-processing steps and models into a single object.
  - [**workflowsets**](https://workflowsets.tidymodels.org/): can create a workflow set that holds multiple workflow objects, allowing users to create and easily fit a large number of models. 
  - [**parsnip**](https://parsnip.tidymodels.org/): a tidy, unified interface to creating models 
  - [**yardstick**](https://yardstick.tidymodels.org/): contains tools for evaluating models

Finally, here are some online resources that can help you with the modeling process:

  - This [**Tidymodels tutorial**](https://www.tidymodels.org/start/) written by the Tidymodels team
  - This [**workshop on Tidymodels**](https://workshops.tidymodels.org/) written by the Tidymodels team
  - This [**workshop on Tidymodels**](https://apreshill.github.io/tidymodels-it/) written by the Posit Team (The company behind RStudio)
  - This [**online course on supervised machine learning**](https://supervised-ml-course.netlify.app/) written by the Tidymodels team

## First phase: Random Forest Model

In this phase, you will build a simple random forest model, by:

  - Splitting the data in training and testing sets
```{r simple_model-training_testing_set}
set.seed(123)

train_indices <- createDataPartition(df$Diagnosis, p = 0.7, list = FALSE)
training_set <- as.data.frame(df[train_indices, ])
testing_set <- as.data.frame(df[-train_indices, ])
```


```{r simple_model-checking_train-test_ratio}
#just curious
prop_train <- table(training_set$Diagnosis) / nrow(training_set)
prop_test <- table(testing_set$Diagnosis) / nrow(testing_set)
orig <- table(df$Diagnosis) / nrow(df)

cat("Proportions in Training Set:\n", paste(names(prop_train), prop_train, sep = ": "), "\n\n")
cat("Proportions in Testing Set:\n", paste(names(prop_test), prop_test, sep = ": "), "\n\n")
cat("Proportions in OG set:\n", paste(names(orig), orig, sep = ": "), "\n\n")
#close enough to the split of Diagnosis in the original set, appears validly split into test and training
#still doesnt account for the split of study, training, other variables (could still be a bad training or testing set)
```

  - Training a random forest model on the training set
```{r training_simple_model}
rf_model <- rand_forest(
  mode = "classification",
  mtry = NULL, 
  trees = 100, 
  min_n = 5
) %>%
  set_engine("ranger") %>% 
  fit(Diagnosis ~ ., data = training_set) #uses all as predictors?, is this a simple random forest model?
```

  - Testing the model's predictions on the testing set
```{r testing_simple_model}
predictions <- predict(rf_model, new_data = testing_set)
```

  - Building the confusion matrix
```{r confusion_matrix_simple_model}
conf_matrix <- confusionMatrix(predictions$.pred_class, testing_set$Diagnosis)
conf_matrix
```

  - Compiling performance metrics of your own choosing.
```{r testing_simple_model_metrics}
#a bunch of metrics from carot package
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Precision"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- (2 * precision * recall) / (precision + recall)
specificity <- conf_matrix$byClass["Specificity"]
roc_auc <- roc(testing_set$Diagnosis, as.numeric(predictions$.pred_class))$auc
#fix this error

#metrics, dont understand all of them yet, i.e. Precision = True Positives/(true positives + false positives)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
print(paste("Precision:", round(precision, 2)))
print(paste("Recall:", round(recall, 2)))
print(paste("F1 Score:", round(f1_score, 2)))
print(paste("Specificity:", round(specificity, 2)))
print(paste("AUC-ROC:", round(roc_auc, 2)))
#70% seems ok for accuracy but could be better?
```

## Second phase: Forest Engineering

In this section, you will try to optimize the performance of the model developed in the previous phase by adding a new random forest model, upgraded with feature engineering and parameter tuning procedures of your own choosing.
#feature engineering!
#parameter tuning procedures!

```{r optimize_model}
rf_model2 <- rand_forest(
    mode = "classification"
  )

predictors <- c(
  "speech_rate",
  "average_syllable_duration",
  "n_syllables",
  "p_max",
  "p_iqr"
)

rf_fit <- rf_model2 %>% fit_xy(
  training_set[, predictors], 
  training_set$Diagnosis
)

predicted <- testing_set %>% 
  mutate(
    Predicted = predict(rf_fit, new_data = testing_set[, predictors])$.pred_class
  )

conf_matrix <- confusionMatrix(data = factor(predicted$Predicted), reference = factor(testing_set$Diagnosis))
conf_matrix
#did the accuracy just get worse?

#a bunch of metrics from carot package
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Precision"]
recall <- conf_matrix$byClass["Sensitivity"]
f1_score <- (2 * precision * recall) / (precision + recall)
specificity <- conf_matrix$byClass["Specificity"]
roc_auc <- roc(testing_set$Diagnosis, as.numeric(predictions$.pred_class))$auc

print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
print(paste("Precision:", round(precision, 2)))
print(paste("Recall:", round(recall, 2)))
print(paste("F1 Score:", round(f1_score, 2)))
print(paste("Specificity:", round(specificity, 2)))
print(paste("AUC-ROC:", round(roc_auc, 2)))
#accuracy lower than original model, why?

```

## Third phase: Another Algorithm

For this final part, add a supervised algorithm to the workflow set and compare its performance to the previous ones. Here again, you are free to choose any algorithm, but it its important that we know what you're doing and why you are doing it. In other words, tell us a bit about the algorithm you're using and why you chose it.

For a detailed list of the model types, engines, and arguments that can be used with the tidymodels framework, have a look here https://www.tidymodels.org/find/parsnip/#models


```{r new_model}

# Copy and paste this code chunk to add further code to this section
# Remember to change the name of the chunk!

```


# Discussion: Methodology and Results

Finally, briefly summarize and discuss the methodological choices you've made throughout as well as the results obtained at the different modeling stages. In particular, I would like to get your input as regards to the following questions:

  - Based on the performance evaluation of your models, do you think the second and third phase of the third section were worth the extra effort? Was any model successful in diagnosing schizophrenia from voice?
  - How do the predictive models you built relate to the descriptive analysis conducted in the second section?
  - What is the explanatory scope of the analyses and procedures you conducted here, if any?

> Write your report here
